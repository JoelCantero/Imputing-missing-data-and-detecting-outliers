---
title: "Imputing missing data and detecting outliers"
author: "Marc Mendez & Joel Cantero Priego"
date: "28/2/2019"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this exercise we will use the Russet data set. In 1964 Russet tried to find the relation between political instability of countries and the economical and agricultural inequality. Russett collected this data to study relationships between Agricultural Inequality, Industrial Development and Political Instability. Russett's hypotheses can be formulated as follows: It is difficult for a country to escape dictatorship when its agricultural inequality is above-average and its industrial
development below-average.

The collected data refer to 47 countries and 9 variables on the period after the Second World War (???1945-1962). The Russett data set (Russett, 1964) are studied in Gifi (1990).
 
First of all, we are going to read 'Russet_ineqdata.txt' and put it to X variable. We have to set some parameters to read.table to function to indicate that:
 
1. This file has a header (header=T).
2. The rows file are separated by tabulation (sep='\t').
3. The first column contains the row names, they are not an attribute (row.names=1).

```{r cars}
X <- read.table('Russet_ineqdata.txt', header=T, sep='\t', row.names=1)
```

We can observe that our X dataframe has 47 rows and 9 attributes:

1. **Gini**: a double variable that indicates the concentration index.
2. **farm**: a double variable that indicates the % of small farmers with 50% of land.
3. **rent**: a double variable that indicates the % of farmers not cultivating their own land.
4. **Gnpr**: a integer variable that indicates the GNP per capita ($ 1955).
5. **Laboagr**: an integer attribute that indicates the % of activate population in Agriculture.
6. **Instab**: a double attribute that indicates the total number of prime ministers during 1945-1961.
7. **ecks**: an integer attribute that indicates the index of violent conflicts during 1946-1961.
8. **Death**: an integer attribute that indicates the deaths during demonstrations 1950-1962.
9. **demo**: an integer attribute that indicates if the country is stable (1), instable (2), or a dictatorship (3).

### Impute missing values and save the result obtained.

We have observed that our dataset has some NA values (missing values) that we have to impute them. 

Ignoring missing data can seriously bias the results. How can we imput them?

Before to start, we have to identify them. In this case, we have realized that our missing data contains the 'NA' string. We will use md.pattern function that is useful for investigating any structure of missing observations in the data. This mentioned function pertains to mice package. So, first of all we have to install and activate this package.

```{r pressure, echo=FALSE, message=FALSE}
#Installing mice package
#install.packages('mice')
library(mice)
```

```{r message=FALSE}
# Identifying missing values with a plot.
md.pattern(X)
```

Afterward, we are going to dealing with these missing values. We can do it in 5 different ways:

#### Ignore records with missing values (Listwise deletion).
The easiest way is to delete these records that have missing values. For this purpose, we will use complete.cases function tthat return a logical vector indicating which rows are complete, i.e., have no missing values.

```{r ignoreRecords}
#Ignore records with missing values.
X.ignoreRecords <-  X[complete.cases(X),]
```

Thanks to this function, we will have our dataset without 4 rows containing missing values. It means that our new dataset will contain 43 records instead of the 47 original ones. 

Due our dataset is very small, this solution is not good for us; deleting 4 rows means that we are deleting almost 10% of our data. For this reason, we will not apply this solution.

### Unconditional mean imputation
This treatment of missing values consists on substitute every missing value by the corresponding global mean of the variable.

As we have seen in the previous section, just two columns are affected by missing values: rent and ecks. We have to calculate the mean of each attribute (dismissing the NA's values, so we are going to use X.ignoreRecords to calculate each mean). Once we have calculated these two values, we have to put them instead of NA's values.

```{r meanImputation}
X.Mean <- X
X.Mean$Rent[is.na(X.Mean$Rent)] <- mean(X$Rent, na.rm = TRUE)
X.Mean$ecks[is.na(X.Mean$ecks)] <- mean(X$ecks, na.rm = TRUE)
```

### Regression imputation
In regression imputation we substitute every missing valueby the predicted value from a multiple regression. In this case, we have to create a lineal model for every attribute that is missing in our dataset, specifically: Rent and ecks.

Once we have our two models, we predict the NA values considering the another attributes of the country (row).

```{r regressionImputation}
X.RegressionImputation <- X

RentModel <- lm(Rent ~ ., X)
EcksModel <- lm(ecks ~ ., data = X)

X.RegressionImputation$Rent[is.na(X$Rent)] <- predict(RentModel, X.RegressionImputation[is.na(X$Rent),])

X.RegressionImputation$ecks[is.na(X$ecks)] <- predict(EcksModel, X.RegressionImputation[is.na(X$ecks),])
```

### Knn imputation
KnnImputation function pertains to DMwR library (we must install and activate it). For every individual containing a missing value in a specific variable, we find another individual with minimal distance to the previous one with complete information. Then transfer (copy) the value of the specific variable, of the second individual to the first one.

```{r knnImputation}
# Installing and activate DMwR package
#install.packages('DMwR')
library(DMwR)

X.knn <- knnImputation(X, k = 7)
```

### Imputation by chained equations
It imputes the missing values of the variable from the predicted values of the regression of the current variable with the remaining ones (similar to lineal regression imputation).
```{r imputationByChainedEquations}
# Installing and activate MICE package
#install.packages('mice')
library(mice)
X.mice <- complete(mice(X, m = 1), 1)
row.names(X.mice) <- row.names(X)
```

### Imputation by random Forests
Random Forests impututes for every variable with missing values the missing values of the variable from the predicted values from the random forest of the individuals with the current variable as response using the remaining ones as predictors. 

```{r imputationByRandomForests, message=FALSE}
#install.packages('missForest')
library(missForest)

X.missForest <- missForest(X)$ximp

```
### Comparing different imputation values
Now we have some ways to impute our NA values, we are going to compare for every NA value the different imputations we have built (discarding ignoring values method).

We can observe that Miss Forest, KNN and Regression Imputation have similar values instead of the other ones.

To conclude, we will use one of these three methods if we have to impute some NA values in our datasets. We discard mean imputation because it does not count the other values of our instance, and it is a bad value.

```{r comparation, message=FALSE}
X.compare <- rbind(X.Mean[is.na(X)], X.RegressionImputation[is.na(X)], X.knn[is.na(X)], X.mice[is.na(X)], X.missForest[is.na(X)] )
row.names(X.compare) <- c('Mean', 'RegressionImputation', 'KNN', 'Mice', 'MissForest')
X.compare
```

###Dealing with univariate outliers
On this part we are asked to take one of the previous datasets and use them to detect and correct outliers. We will use the miss forest dataset. First step is to detect univariate outliers, this can be done with boxplots.

```{r boxplot, message=FALSE}
boxplot(X.outliers$Gini, horizontal = TRUE, col = 'red')
boxplot(X.outliers$farm, horizontal = TRUE, col = 'red')
boxplot(X.outliers$Rent, horizontal = TRUE, col = 'red')
boxplot(X.outliers$Gnpr, horizontal = TRUE, col = 'red')
boxplot(X.outliers$Laboagr, horizontal = TRUE, col = 'red')
boxplot(X.outliers$Instab, horizontal = TRUE, col = 'red')
boxplot(X.outliers$Death, horizontal = TRUE, col = 'red')
```


By analyzing the boxplots, we can see that some of the values are univariate outliers. This can be due to an anomaly lecture or an error when takin the data. In our case we can try to solve this by making them as an NA value and using an imput method to insert a more accurate value. We can do this more than once until we get all outliers corrected.

```{r, message=FALSE}
X.outliers$Instab[X.outliers$Instab < summary(X.outliers$Instab)[2]-1.5*(summary(X.outliers$Instab)[5] - summary(X.outliers$Instab))[2]] <- NA
X.outliers <- missForest(X.outliers)$ximp
boxplot(X.outliers$Instab, horizontal = TRUE, col = 'red')
```

As we can see in this example, we detect univariate outliers in the boxplot. To solve it we transform them to NA and after assign a value. Once this is done we need to check if there are still outliers. 

###Dealing with multivariate outliers
The next step once we have detected and corrected the univariate outliers, it is time to work on the multivariate. This means not only comparing each of the features of the dataset individualy but doing it with the Mahalanobis distances.

For doing this we will use the moutliers function from the chemometrics package. With this function we obtain three values: the classical Mahalanobis distance, the robust Mahalanobis distance and the cutoff from which every value is an outlier.



Another way to detect multivariate outliers is to use the Local Outlier Factor algorithm. Is is usefull for identifying density-based local outliers.

```{r LOF outliers}
nameOutliers
```
Here we picked the 5 values with highest LOF as potential outliers. 



## References
* Russett B.M. (1964), Inequality and Instability: The Relation of Land Tenure to Politics, World
Politics 16:3, 442-454.
* Gifi, A. (1990), Nonlinear multivariate analysis, Chichester: Wiley.

