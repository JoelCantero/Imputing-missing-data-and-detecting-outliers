---
title: "Imputing missing data and detecting outliers"
author: "Marc Mendez & Joel Cantero Priego"
date: "28/2/2019"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

In this exercise we will use the Russet data set. In 1964 Russet tried to find the relation between political instability of countries and the economical and agricultural inequality. Russett collected this data to study relationships between Agricultural Inequality, Industrial Development and Political Instability. Russett's hypotheses can be formulated as follows: It is difficult for a country to escape dictatorship when its agricultural inequality is above-average and its industrial
development below-average.

The collected data refer to 47 countries and 9 variables on the period after the Second World War (???1945-1962). The Russett data set (Russett, 1964) are studied in Gifi (1990).
 
First of all, we are going to read 'Russet_ineqdata.txt' and put it to X variable. We have to set some parameters to read.table to function to indicate that:
 
1. This file has a header (header=T).
2. The rows file are separated by tabulation (sep='\t').
3. The first column contains the row names, they are not an attribute (row.names=1).

```{r cars}
X <- read.table('Russet_ineqdata.txt', header=T, sep='\t', row.names=1)
```

We can observe that our X dataframe has 47 rows and 9 attributes:

1. **Gini**: a double variable that indicates the concentration index.
2. **farm**: a double variable that indicates the % of small farmers with 50% of land.
3. **rent**: a double variable that indicates the % of farmers not cultivating their own land.
4. **Gnpr**: a integer variable that indicates the GNP per capita ($ 1955).
5. **Laboagr**: an integer attribute that indicates the % of activate population in Agriculture.
6. **Instab**: a double attribute that indicates the total number of prime ministers during 1945-1961.
7. **ecks**: an integer attribute that indicates the index of violent conflicts during 1946-1961.
8. **Death**: an integer attribute that indicates the deaths during demonstrations 1950-1962.
9. **demo**: an integer attribute that indicates if the country is stable (1), instable (2), or a dictatorship (3).

### Impute missing values and save the result obtained.

We have observed that our dataset has some NA values (missing values) that we have to impute them. 

Ignoring missing data can seriously bias the results. How can we imput them?

Before to start, we have to identify them. In this case, we have realized that our missing data contains the 'NA' string. We will use md.pattern function that is useful for investigating any structure of missing observations in the data. This mentioned function pertains to mice package. So, first of all we have to install and activate this package.

```{r pressure, echo=FALSE, message=FALSE}
#Installing mice package
#install.packages('mice')
library(mice)
```

```{r message=FALSE, results='hide'}
# Identifying missing values with a plot.
md.pattern(X)
```

Afterward, we are going to dealing with these missing values. We can do it in 5 different ways:

#### Ignore records with missing values (Listwise deletion).
The easiest way is to delete these records that have missing values. For this purpose, we will use complete.cases function tthat return a logical vector indicating which rows are complete, i.e., have no missing values.

```{r ignoreRecords}
#Ignore records with missing values.
X.ignoreRecords <-  X[complete.cases(X),]
```

Thanks to this function, we will have our dataset without 4 rows containing missing values. It means that our new dataset will contain 43 records instead of the 47 original ones. 

Due our dataset is very small, this solution is not good for us; deleting 4 rows means that we are deleting almost 10% of our data. For this reason, we will not apply this solution.

### Unconditional mean imputation
This treatment of missing values consists on substitute every missing value by the corresponding global mean of the variable.

As we have seen in the previous section, just two columns are affected by missing values: rent and ecks. We have to calculate the mean of each attribute (dismissing the NA's values, so we are going to use X.ignoreRecords to calculate each mean). Once we have calculated these two values, we have to put them instead of NA's values.

```{r meanImputation}
X.Mean <- X
X.Mean$Rent[is.na(X.Mean$Rent)] <- mean(X$Rent, na.rm = TRUE)
X.Mean$ecks[is.na(X.Mean$ecks)] <- mean(X$ecks, na.rm = TRUE)
```

### Regression imputation
In regression imputation we substitute every missing valueby the predicted value from a multiple regression. In this case, we have to create a lineal model for every attribute that is missing in our dataset, specifically: Rent and ecks.

Once we have our two models, we predict the NA values considering the another attributes of the country (row).

```{r regressionImputation}
X.RegressionImputation <- X

RentModel <- lm(Rent ~ ., X)
EcksModel <- lm(ecks ~ ., data = X)

X.RegressionImputation$Rent[is.na(X$Rent)] <- predict(RentModel, X.RegressionImputation[is.na(X$Rent),])

X.RegressionImputation$ecks[is.na(X$ecks)] <- predict(EcksModel, X.RegressionImputation[is.na(X$ecks),])
```

### Knn imputation
KnnImputation function pertains to DMwR library (we must install and activate it). For every individual containing a missing value in a specific variable, we find another individual with minimal distance to the previous one with complete information. Then transfer (copy) the value of the specific variable, of the second individual to the first one.

```{r knnImputation}
# Installing and activate DMwR package
#install.packages('DMwR')
library(DMwR)

X.knn <- knnImputation(X, k = 7)
```

### Imputation by chained equations
It imputes the missing values of the variable from the predicted values of the regression of the current variable with the remaining ones (similar to lineal regression imputation).
```{r imputationByChainedEquations}
# Installing and activate MICE package
#install.packages('mice')
library(mice)
X.mice <- complete(mice(X, m = 1), 1)
row.names(X.mice) <- row.names(X)
```

### Imputation by random Forests
Random Forests impututes for every variable with missing values the missing values of the variable from the predicted values from the random forest of the individuals with the current variable as response using the remaining ones as predictors. 

```{r imputationByRandomForests, message=FALSE}
#install.packages('missForest')
library(missForest)

X.missForest <- missForest(X)$ximp

```
### Comparing different imputation values
Now we have some ways to impute our NA values, we are going to compare for every NA value the different imputations we have built (discarding ignoring values method).

We can observe that Miss Forest, KNN and Regression Imputation have similar values instead of the other ones.

To conclude, we will use one of these three methods if we have to impute some NA values in our datasets. We discard mean imputation because it does not count the other values of our instance, and it is a bad value.

```{r comparation, message=FALSE, echo=FALSE}
X.compare <- rbind(X.Mean[is.na(X)], X.RegressionImputation[is.na(X)], X.knn[is.na(X)], X.mice[is.na(X)], X.missForest[is.na(X)] )
row.names(X.compare) <- c('Mean', 'RegressionImputation', 'KNN', 'Mice', 'MissForest')
X.compare
```
```{r echo=FALSE}
#install.packages(ggplot2)
library(ggplot2)
temp <- X
temp[,10] <- NA
names(temp)[10] <- "Index"
for (i in 1:nrow(temp))
  {
    temp[i,10] <- i
  }
#Plotting Rent data imputed to see the comparation
ggplot(data = X.knn, mapping = aes(y = X.knn$Rent, x = temp$Index)) + 
  geom_point(data=X, aes(y = X.knn$Rent, x = temp$Index, colour="Data") , size=1) +
  geom_point(data=X.Mean, aes(y = X.Mean$Rent[2], x = temp$Index[2], colour="Mean"), size=3) +
  geom_point(data=X.Mean, aes(y = X.Mean$Rent[30], x = temp$Index[30], colour="Mean"), size=3) +
  geom_point(data=X.Mean, aes(y = X.Mean$Rent[35], x = temp$Index[35], colour="Mean"), size=3) +
  geom_point(data=X.RegressionImputation, aes(y = X.RegressionImputation$Rent[2], x = temp$Index[2], colour="Regression"), size=3) +
  geom_point(data=X.RegressionImputation, aes(y = X.RegressionImputation$Rent[30], x = temp$Index[30], colour="Regression"), size=3) +
  geom_point(data=X.RegressionImputation, aes(y = X.RegressionImputation$Rent[35], x = temp$Index[35], colour="Regression"), size=3) +
  geom_point(data=X.knn, aes(y = X.knn$Rent[2], x = temp$Index[2], colour="Knn"), size=3) +
  geom_point(data=X.knn, aes(y = X.knn$Rent[30], x = temp$Index[30], colour="Knn"), size=3) +
  geom_point(data=X.knn, aes(y = X.knn$Rent[35], x = temp$Index[35], colour="Knn"), size=3) +
  geom_point(data=X.missForest, aes(y = X.missForest$Rent[2], x = temp$Index[2], colour="MissForest"), size=3) +
  geom_point(data=X.missForest, aes(y = X.missForest$Rent[30], x = temp$Index[30], colour="MissForest"), size=3) +
  geom_point(data=X.missForest, aes(y = X.missForest$Rent[35], x = temp$Index[35], colour="MissForest"), size=3) +
  geom_point(data=X.mice, aes(y = X.mice$Rent[2], x = temp$Index[2], colour="Mice"), size=3) +
  geom_point(data=X.mice, aes(y = X.mice$Rent[30], x = temp$Index[30], colour="Mice"), size=3) +
  geom_point(data=X.mice , aes(y = X.mice$Rent[35], x = temp$Index[35], colour="Mice"), size=3) +
  xlab("Index") + ylab("Rent") +
  ggtitle("Imputation of the Rent Values")

#Plotting ecks data imputed to see the comparation
ggplot(data = X.knn, mapping = aes(y = X.knn$ecks, x = temp$Index)) + 
  geom_point(data=X, aes(y = X.knn$ecks, x = temp$Index, colour="Data") , size=1) +
  geom_point(data=X.Mean, aes(y = X.Mean$ecks[31], x = temp$Index[31], colour="Mean"), size=3) +
  geom_point(data=X.RegressionImputation, aes(y = X.RegressionImputation$ecks[31], x = temp$Index[31], colour="Regression"), size=3) +
  geom_point(data=X.knn, aes(y = X.knn$ecks[31], x = temp$Index[31], colour="Knn"), size=3) +
  geom_point(data=X.missForest, aes(y = X.missForest$ecks[31], x = temp$Index[31], colour="MissForest"), size=3) +
  geom_point(data=X.mice, aes(y = X.mice$ecks[31], x = temp$Index[31], colour="Mice"), size=3) +
  xlab("Index") + ylab("ecks") +
  ggtitle("Imputation of the ecks Values")
```



###Dealing with univariate outliers

On this part we are asked to take one of the previous datasets and use them to detect and correct(if necessary) outliers. We will use the miss forest dataset. First step is to detect univariate outliers, this can be done with boxplots.Then we should consider if they are rare observations or errors when collecting the data. 

For the first case we should only consider them and try to understand where they come from. For the errors, we should delete them as the value we see is not a real value and imput with some of the previous methods. For example miss Forest.


```{r, message=FALSE, eval =FALSE}
X.outliers <- X.missForest
X.outliers$Instab[X.outliers$Instab < summary(X.outliers$Instab)[2]-1.5*(summary(X.outliers$Instab)[5] - summary(X.outliers$Instab))[2]] <- NA
X.outliers <- missForest(X.outliers)$ximp
boxplot(X.outliers$Instab, horizontal = TRUE, col = 'red')
```

This is an example of how we are correcting errors, first we detect univariate outliers in the boxplot. To solve it, we transform them to NA and after assign a value. Once this is done we need to check if there are still outliers. 
Maybe after this steps we still have some outliers, because the 1st Q. and the 3rd Q. have changed so this can make new outliers appear. We would have to reapeat all the process to check if they are rare values or errors.

###Dealing with multivariate outliers

Once you are sure your data has no errors and everything is correct, it is time to work on the multivariate analysis. Data is multivariate and detecting univariate outliers does not imply multivariate detection. So a good procedure is to calculate the Mahalanobis distance to see for each country the distance between a point and a distribution.

For doing this we will use the moutliers function from the chemometrics package. With this function we obtain three values: the classical Mahalanobis distance, the robust Mahalanobis distance and the cutoff from which every value is an outlier(only for the Classical distance).

```{r plot Robust distance, message = FALSE, echo= FALSE}
#install.packages('chemometrics')
library(chemometrics) 



X.multivariantOutliers <- X.missForest
temp2 <- Moutlier(X.multivariantOutliers, quantile = 0.995, plot = FALSE)
X.multivariantOutliers <- cbind(X.multivariantOutliers, temp2$md, temp2$rd, temp2$cutoff)
names(X.multivariantOutliers)[10] <- "Classical" 
names(X.multivariantOutliers)[11] <- "Robust" 
names(X.multivariantOutliers)[12] <- "Cutoff"
ggplot(data = X.multivariantOutliers ,mapping = aes(y = X.multivariantOutliers$Robust, x = temp$Index, label = rownames(X.multivariantOutliers) )) +
  geom_point(data= X.multivariantOutliers, aes(y = X.multivariantOutliers$Robust, x = temp$Index, colour="Data") , size=1) +
  xlab("Index") + ylab("Robust") +
  geom_text(hjust=0, vjust=0) +
  ggtitle("Robust Distance")
```

The problem with Classical Mahalanobis distance can be influenced by univariate outliers. To avoid this we will use the Robus Mahalanobis distance which shows more clearly who are the outliers by estimating the centrallity and the covariance matrix. The problem with this distance could be on where to set the cutoff. So for doing this the plot is very usefull. 

As we see on the example above, we can see as a clear outlier Cuba but the others are not so clear so we will set the cutoff arround 1000. The main reason is because almost all values have very low distance, and some of them are at most 500 so the three top outliers are far away from the following values. This means, the outlier with lower value is arround 1000 and the non-outlier data with higher value are on 500 which is 1/2 of the value of the outlier

With that said, the Robust shows clearly that Cuba is a multivariate outlier and we can also consider Sud-Vietnam and Bolicie as well. The other ones are not so clear, so we will consider only this three as potential outliers.

Another option, is to plot Classical distance vs Robust distance. By doing this we will have a more accurate solution as the outliers found by both methods give you stronger base to say they are outliers. This example applies for Cuba, is a clear outlier in Classical and also clear in Robust so we can say with high confidence it is an outlier.

Another way to detect multivariate outliers is to use the Local Outlier Factor algorithm. Is is usefull for identifying density-based local outliers. It is based on measuring the local deviation of a given data point with respect to its neighbours.

```{r LOF outliers, message= FALSE, echo =FALSE}
#install.packages('DMwR')
library(DMwR)
X.multivariantOutliers <- X.missForest
outlier.scores <- lofactor(X.multivariantOutliers, k=5)
# pick top 5 outliers
outliers <- order(outlier.scores, decreasing=T)[1:5]
# who are outliers
nameOutliers<- list()
for (i in 1:nrow(X.multivariantOutliers))
{
  if (i %in% outliers) nameOutliers <- rbind(nameOutliers, row.names(X.multivariantOutliers)[i])
}
nameOutliers
```
Here we picked the 5 top values with highest LOF as potential outliers. 

Then we should compare the results obtained by Mahalanobis distance and the ones obtained by LOF algorithm. If a country is present in all of the methods then, is an outlier. So in our case, Cuba, Bolivie and Sud-Vietnam are present in both methods so that is a strong reason to say they are outliers. For the other two, we would have to study them(Canada and Estats-Units) as well as the outliers obtained by the Mahalanobis Robust distance.




## References
* Russett B.M. (1964), Inequality and Instability: The Relation of Land Tenure to Politics, World
Politics 16:3, 442-454.
* Gifi, A. (1990), Nonlinear multivariate analysis, Chichester: Wiley.

